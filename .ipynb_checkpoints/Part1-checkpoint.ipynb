{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('wbdc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, 0] = df.iloc[:, 0].replace({'M' : 1, 'B' : 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_df(df):\n",
    "    df = shuffle(df) \n",
    "    \n",
    "    M_df = df[df['Diagnosis']==1]\n",
    "    B_df = df[df['Diagnosis']==0]\n",
    "    \n",
    "    M_test = M_df.iloc[:int(0.20*len(M_df)), :]\n",
    "    M_train = M_df.iloc[int(0.20*len(M_df)):, :]\n",
    "    \n",
    "    B_test = B_df.iloc[:int(0.20*len(B_df)), :]\n",
    "    B_train = B_df.iloc[int(0.20*len(B_df)):, :]\n",
    "    \n",
    "    train_df = pd.concat([M_train, B_train])\n",
    "    test_df = pd.concat([M_test, B_test])\n",
    "    x_train = train_df.iloc[:, 1:]\n",
    "    y_train = train_df.iloc[:, 0]\n",
    "    x_test = test_df.iloc[:, 1:]\n",
    "    y_test = test_df.iloc[:, 0]\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Supervised Learning: Train an L1-penalized SVM to classify the data. Use 5 fold cross validation to choose the penalty parameter. Use normalized data. Report the average accuracy, precision, recall, F-score, and AUC, for both training and test sets over your M runs. Plot the ROC and report the confusion matrix for training and testing in one of the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\n",
    "    auc_ = metrics.auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr, label=' (area = {:.3f})'.format(auc_))\n",
    "    plt.xlabel('False Positive rate')\n",
    "    plt.ylabel('True Positive rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_c_range(x_train, y_train):\n",
    "    c_ = np.logspace(-5, 8, 10)\n",
    "    scores = []\n",
    "    for c in c_:\n",
    "        svc = LinearSVC(penalty='l1', C=c, dual=False)\n",
    "        svc.fit(x_train, y_train)\n",
    "        scores.append(svc.score(x_train, y_train))\n",
    "    scores = np.array(scores)\n",
    "    ind = np.argwhere(scores > 0.9).flatten()\n",
    "    c_1 = c_[ind[0]]\n",
    "    c_2 = c_[ind[-1]]\n",
    "    return c_1, c_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_results(results):\n",
    "    print('Avg. Train Accuracy {}'.format(np.mean(results.iloc[:, 0])))\n",
    "    print('Avg. Train Precision {}'.format(np.mean(results.iloc[:, 1])))\n",
    "    print('Avg. Train Recall {}'.format(np.mean(results.iloc[:, 2])))\n",
    "    print('Avg. Train F-1 score {}'.format(np.mean(results.iloc[:, 3])))\n",
    "    print('Avg. Train AUC {}'.format(np.mean(results.iloc[:, 4])))\n",
    "\n",
    "    print('\\nTest Accuracy {}'.format(np.mean(results.iloc[:, 5])))\n",
    "    print('Test Precision {}'.format(np.mean(results.iloc[:, 6])))\n",
    "    print('Test Recall {}'.format(np.mean(results.iloc[:, 7])))\n",
    "    print('Test F1 Score {}'.format(np.mean(results.iloc[:, 8])))\n",
    "    print('Test AUC {}'.format(np.mean(results.iloc[:, 9])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from math import log10\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use normalized data. Report the average accuracy, precision, recall, F-score, and AUC, for both training and test sets \n",
    "#over your M runs. Plot the ROC and report the confusion matrix for training and testing in one of the runs.\n",
    "results1 = []\n",
    "\n",
    "for i in tqdm(range(15)):\n",
    "    print('Run Number {}'.format(i))\n",
    "    each = []\n",
    "    x_train, y_train, x_test, y_test = train_test_df(df)\n",
    "    scaler = preprocessing.Normalizer()\n",
    "    x_train_ = scaler.fit_transform(x_train)\n",
    "    x_test_ = scaler.transform(x_test)\n",
    "    \n",
    "    c_l, c_h = get_c_range(x_train_, y_train)\n",
    "    parameters = {'C':np.logspace(log10(c_l), log10(c_h), 20)}\n",
    "    \n",
    "    scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "    \n",
    "    svc = LinearSVC(penalty='l1', dual=False)\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    clf = GridSearchCV(svc, parameters, cv=kf, scoring=scoring, refit='roc_auc', return_train_score=True)\n",
    "    clf.fit(x_train_, y_train)\n",
    "    \n",
    "    results = clf.cv_results_\n",
    "    \n",
    "    each.append(round(np.mean(results['mean_train_accuracy']), 2))\n",
    "    each.append(round(np.mean(results['mean_train_precision']), 2))\n",
    "    each.append(round(np.mean(results['mean_train_recall']), 2))\n",
    "    each.append(round(np.mean(results['mean_train_f1']), 2))\n",
    "    each.append(round(np.mean(results['mean_train_roc_auc']), 2))\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    sv = clf.best_estimator_\n",
    "    y_pred = sv.predict(x_test_)\n",
    "    \n",
    "    each.append(round(metrics.accuracy_score(y_test, y_pred), 2))\n",
    "    each.append(round(metrics.recall_score(y_test, y_pred), 2))\n",
    "    each.append(round(metrics.precision_score(y_test, y_pred), 2))\n",
    "    each.append(round(metrics.f1_score(y_test, y_pred), 2))\n",
    "    each.append(round(metrics.roc_auc_score(y_test, y_pred), 2))\n",
    "    \n",
    "    print('Avg. Train Accuracy {}'.format(round(np.mean(results['mean_train_accuracy']), 2)))\n",
    "    print('Test Accuracy {}'.format(round(metrics.accuracy_score(y_test, y_pred), 2)))\n",
    "    \n",
    "    if(i == 14):\n",
    "        print('\\nFor Run Number {}'.format(i))\n",
    "        cnf_train = metrics.confusion_matrix(y_train, sv.predict(x_train_))\n",
    "        print('Train Confusion Matrix')\n",
    "        print(cnf_train)\n",
    "        #roc_curve(sv, x_train_, y_train)\n",
    "\n",
    "        cnf_test = metrics.confusion_matrix(y_test, y_pred)\n",
    "        print('\\nTest Confusion Matrix')\n",
    "        print(cnf_test)\n",
    "        #roc_curve(sv, x_test_, y_test)\n",
    "    results1.append(each)\n",
    "    print('\\n')\n",
    "\n",
    "return_results(pd.DataFrame(results1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Semi-Supervised Learning/ Self-training: select 50% of the positive\n",
    "class along with 50% of the negative class in the training set as labeled data\n",
    "and the rest as unlabelled data. You can select them randomly.   \n",
    "   \n",
    "A. Train an L1-penalized SVM to classify the labeled data Use normalized\n",
    "data. Choose the penalty parameter using 5 fold cross validation.   \n",
    "   \n",
    "B. Find the unlabeled data point that is the farthest to the decision boundary\n",
    "of the SVM. Let the SVM label it (ignore its true label), and add it to\n",
    "the labeled data, and retrain the SVM. Continue this process until all\n",
    "unlabeled data are used. Test the final SVM on the test data andthe\n",
    "average accuracy, precision, recall, F-score, and AUC, for both training\n",
    "and test sets over your M runs. Plot the ROC and report the confusion\n",
    "matrix for training and testing in one of the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_df(df):\n",
    "    df = shuffle(df) \n",
    "    M_df = df[df['Diagnosis']==1]\n",
    "    B_df = df[df['Diagnosis']==0]\n",
    "    \n",
    "    M_test = M_df.iloc[:int(0.20*len(M_df)), :]\n",
    "    M_train = M_df.iloc[int(0.20*len(M_df)):, :]\n",
    "    \n",
    "    B_test = B_df.iloc[:int(0.20*len(B_df)), :]\n",
    "    B_train = B_df.iloc[int(0.20*len(B_df)):, :]\n",
    "    \n",
    "    train_df = pd.concat([M_train, B_train])\n",
    "    test_df = pd.concat([M_test, B_test])\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lab_unlab_split(df):\n",
    "    df = shuffle(df) \n",
    "    M_df = df[df['Diagnosis']==1]\n",
    "    B_df = df[df['Diagnosis']==0]\n",
    "    \n",
    "    M_test = M_df.iloc[:int(0.50*len(M_df)), :]\n",
    "    M_train = M_df.iloc[int(0.50*len(M_df)):, :]\n",
    "    \n",
    "    B_test = B_df.iloc[:int(0.50*len(B_df)), :]\n",
    "    B_train = B_df.iloc[int(0.50*len(B_df)):, :]\n",
    "    \n",
    "    train_df = pd.concat([M_train, B_train])\n",
    "    test_df = pd.concat([M_test, B_test])\n",
    "    x_train = train_df.iloc[:, :-1]\n",
    "    y_train = train_df.iloc[:, -1]\n",
    "    x_test = test_df.iloc[:, :-1]\n",
    "    y_test = test_df.iloc[:, -1]\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_farthest_points(svc, x_test_):\n",
    "    x_cols = df.columns.values[1:]\n",
    "    y_cols = df.columns.values[0]\n",
    "\n",
    "    dd = pd.DataFrame(x_test_)\n",
    "    y = svc.decision_function(x_test_)\n",
    "    w_norm = np.linalg.norm(svc.coef_)\n",
    "    dist = y / w_norm\n",
    "    d = np.argmax([abs(a) for a in dist])\n",
    "    point = svc.predict([dd.iloc[d, :]])\n",
    "    \n",
    "    return dd.iloc[1,:], point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use normalized data. Report the average accuracy, precision, recall, F-score, and AUC, for both training and test sets \n",
    "#over your M runs. Plot the ROC and report the confusion matrix for training and testing in one of the runs.\n",
    "results2 = []\n",
    "\n",
    "for i in tqdm(range(1)):\n",
    "    print('Run Number {}'.format(i))\n",
    "    each = []\n",
    "    \n",
    "    #Divide the initial df into 80-20 test-train\n",
    "    train_d, test_d = train_test_split_df(df)\n",
    "    \n",
    "    x_train_ = train_d.iloc[:, 1:]\n",
    "    y_train = train_d.iloc[:, 0]\n",
    "    x_test_ = test_d.iloc[:, 1:]\n",
    "    y_test = test_d.iloc[:, 0]\n",
    "    \n",
    "    scaler = preprocessing.Normalizer()\n",
    "    \n",
    "    train_data = pd.DataFrame(scaler.fit_transform(x_train_), columns=df.columns.values[1:])\n",
    "    test_data = pd.DataFrame(scaler.fit_transform(x_test_), columns=df.columns.values[1:])\n",
    "    \n",
    "    train_data['Diagnosis'] = [int(i) for i in y_train]\n",
    "    test_data['Diagnosis'] = [int(j) for j in y_test]\n",
    "    \n",
    "    \n",
    "    #Concat train data, and divide into 50-50 labelled and unlabelled\n",
    "    #train_data = pd.concat([x_train, y_train])\n",
    "    lab_x, lab_y, unlab_x, unlab_y = lab_unlab_split(train_data)\n",
    "    \n",
    "    for j in range(len(unlab_x) + 1):\n",
    "        #Use labelled data as your training data and unlabelled data as your test data\n",
    "        c_l, c_h = get_c_range(lab_x, lab_y)\n",
    "        parameters = {'C':np.logspace(log10(c_l), log10(c_h), 20)}\n",
    "\n",
    "        scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "\n",
    "        svc = LinearSVC(penalty='l1', dual=False)\n",
    "        kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "        clf = GridSearchCV(svc, parameters, cv=kf, scoring=scoring, refit='roc_auc', return_train_score=True)\n",
    "        clf.fit(lab_x, lab_y)\n",
    "\n",
    "        # calculating test accuracy\n",
    "        sv = clf.best_estimator_\n",
    "        xx, yy = get_farthest_points(sv, unlab_x)\n",
    "        lab_x = lab_x.append(pd.Series(xx))\n",
    "        lab_y = lab_y.append(pd.Series(yy))\n",
    "        \n",
    "    results = clf.cv_results_\n",
    "    \n",
    "    each.append(round(np.mean(results['mean_train_accuracy']), 2))\n",
    "    each.append(round(np.mean(results['mean_train_precision']), 2))\n",
    "    each.append(round(np.mean(results['mean_train_recall']), 2))\n",
    "    each.append(round(np.mean(results['mean_train_f1']), 2))\n",
    "    each.append(round(np.mean(results['mean_train_roc_auc']), 2))\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    sv = clf.best_estimator_\n",
    "    y_pred = sv.predict(x_test)\n",
    "    \n",
    "    each.append(round(metrics.accuracy_score(y_test, y_pred), 2))\n",
    "    each.append(round(metrics.recall_score(y_test, y_pred), 2))\n",
    "    each.append(round(metrics.precision_score(y_test, y_pred), 2))\n",
    "    each.append(round(metrics.f1_score(y_test, y_pred), 2))\n",
    "    each.append(round(metrics.roc_auc_score(y_test, y_pred), 2))\n",
    "    \n",
    "    print('Avg. Train Accuracy {}'.format(round(np.mean(results['mean_train_accuracy']), 2)))\n",
    "    print('Test Accuracy {}'.format(round(metrics.accuracy_score(y_test, y_pred), 2)))\n",
    "    \n",
    "    if(i == 0):\n",
    "        print('For Run Number {}'.format(i))\n",
    "        cnf_train = metrics.confusion_matrix(y_train, sv.predict(x_train))\n",
    "        print('Train Confusion Matrix')\n",
    "        print(cnf_train)\n",
    "        #roc_curve(sv, x_train, y_train)\n",
    "\n",
    "        cnf_test = metrics.confusion_matrix(y_test, y_pred)\n",
    "        print('\\nTest Confusion Matrix')\n",
    "        print(cnf_test)\n",
    "        #roc_curve(sv, x_test, y_test)\n",
    "    \n",
    "    results2.append(each)\n",
    "    print('\\n')\n",
    "\n",
    "return_results(pd.DataFrame(results2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
