{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from math import log10\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('wbdc.csv')\n",
    "df = df.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, 0] = df.iloc[:, 0].replace({'M' : 1, 'B' : 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTIONS NEEDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_df(df):\n",
    "    df = shuffle(df) \n",
    "    \n",
    "    M_df = df[df['Diagnosis']==1]\n",
    "    B_df = df[df['Diagnosis']==0]\n",
    "    \n",
    "    M_test = M_df.iloc[:int(0.20*len(M_df)), :]\n",
    "    M_train = M_df.iloc[int(0.20*len(M_df)):, :]\n",
    "    \n",
    "    B_test = B_df.iloc[:int(0.20*len(B_df)), :]\n",
    "    B_train = B_df.iloc[int(0.20*len(B_df)):, :]\n",
    "    \n",
    "    train_df = pd.concat([M_train, B_train])\n",
    "    test_df = pd.concat([M_test, B_test])\n",
    "    x_train = train_df.iloc[:, 1:]\n",
    "    y_train = train_df.iloc[:, 0]\n",
    "    x_test = test_df.iloc[:, 1:]\n",
    "    y_test = test_df.iloc[:, 0]\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve(model, X_test, y_test):\n",
    "    y_pred = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\n",
    "    auc_ = metrics.auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot([0.0, 1.0], [0.0, 1.05], 'k--')\n",
    "    plt.plot(fpr, tpr, label=' (area = {:.3f})'.format(auc_))\n",
    "    plt.xlabel('False Positive rate')\n",
    "    plt.ylabel('True Positive rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_c_range(x_train, y_train):\n",
    "    c_ = np.logspace(-5, 8, 10)\n",
    "    scores = []\n",
    "    for c in c_:\n",
    "        svc = LinearSVC(penalty='l1', C=c, dual=False)\n",
    "        svc.fit(x_train, y_train)\n",
    "        scores.append(svc.score(x_train, y_train))\n",
    "    scores = np.array(scores)\n",
    "    ind = np.argwhere(scores > 0.9).flatten()\n",
    "    c_1 = c_[ind[0]]\n",
    "    c_2 = c_[ind[-1]]\n",
    "    return c_1, c_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_results(results):\n",
    "    print('Avg. Train Accuracy {}'.format(np.mean(results.iloc[:, 0])))\n",
    "    print('Avg. Train Precision {}'.format(np.mean(results.iloc[:, 1])))\n",
    "    print('Avg. Train Recall {}'.format(np.mean(results.iloc[:, 2])))\n",
    "    print('Avg. Train F-1 score {}'.format(np.mean(results.iloc[:, 3])))\n",
    "    print('Avg. Train AUC {}'.format(np.mean(results.iloc[:, 4])))\n",
    "\n",
    "    print('\\nTest Accuracy {}'.format(np.mean(results.iloc[:, 5])))\n",
    "    print('Test Precision {}'.format(np.mean(results.iloc[:, 6])))\n",
    "    print('Test Recall {}'.format(np.mean(results.iloc[:, 7])))\n",
    "    print('Test F1 Score {}'.format(np.mean(results.iloc[:, 8])))\n",
    "    print('Test AUC {}'.format(np.mean(results.iloc[:, 9])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearSVC(x_train, y_train, scoring):\n",
    "    c_l, c_h = get_c_range(x_train, y_train)\n",
    "    parameters = {'C':np.logspace(log10(c_l), log10(c_h), 20)}\n",
    "\n",
    "    svc = LinearSVC(penalty='l1', dual=False)\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    clf = GridSearchCV(svc, parameters, cv=kf, scoring=scoring, refit='roc_auc', return_train_score=True)\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf.best_estimator_, clf.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Supervised Learning: Train an L1-penalized SVM to classify the data. Use 5 fold cross validation to choose the penalty parameter. Use normalized data. Report the average accuracy, precision, recall, F-score, and AUC, for both training and test sets over your M runs. Plot the ROC and report the confusion matrix for training and testing in one of the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use normalized data. Report the average accuracy, precision, recall, F-score, and AUC, for both training and test sets \n",
    "#over your M runs. Plot the ROC and report the confusion matrix for training and testing in one of the runs.\n",
    "results1 = []\n",
    "\n",
    "for i in tqdm(range(30)):\n",
    "    #print('Run Number {}'.format(i))\n",
    "    each = []\n",
    "    x_train, y_train, x_test, y_test = train_test_df(df)\n",
    "    scaler = preprocessing.Normalizer()\n",
    "    x_train_ = scaler.fit_transform(x_train)\n",
    "    x_test_ = scaler.transform(x_test)\n",
    "    \n",
    "    scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "    sv, results = linearSVC(x_train_, y_train, scoring)\n",
    "    \n",
    "    each.append(round(np.mean(results['mean_train_accuracy']), 2))\n",
    "    each.append(round(np.mean(results['mean_train_precision']), 2))\n",
    "    each.append(round(np.mean(results['mean_train_recall']), 2))\n",
    "    each.append(round(np.mean(results['mean_train_f1']), 2))\n",
    "    each.append(round(np.mean(results['mean_train_roc_auc']), 2))\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    y_pred = sv.predict(x_test_)\n",
    "    \n",
    "    each.append(round(metrics.accuracy_score(y_test, y_pred), 2))\n",
    "    each.append(round(metrics.recall_score(y_test, y_pred), 2))\n",
    "    each.append(round(metrics.precision_score(y_test, y_pred), 2))\n",
    "    each.append(round(metrics.f1_score(y_test, y_pred), 2))\n",
    "    each.append(round(metrics.roc_auc_score(y_test, y_pred), 2))\n",
    "    \n",
    "    #print('Avg. Train Accuracy {}'.format(round(np.mean(results['mean_train_accuracy']), 2)))\n",
    "    #print('Test Accuracy {}'.format(round(metrics.accuracy_score(y_test, y_pred), 2)))\n",
    "    \n",
    "    if(i == 29):\n",
    "        print('\\nFor Run Number {}'.format(i))\n",
    "        kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "        calib = CalibratedClassifierCV(sv, cv=kf)\n",
    "        \n",
    "        cnf_train = metrics.confusion_matrix(y_train, sv.predict(x_train_))\n",
    "        print('Train Confusion Matrix')\n",
    "        print(cnf_train)\n",
    "        calib.fit(x_train_, y_train)\n",
    "        roc_curve(calib, x_train_, y_train)\n",
    "\n",
    "        cnf_test = metrics.confusion_matrix(y_test, y_pred)\n",
    "        print('\\nTest Confusion Matrix')\n",
    "        print(cnf_test)\n",
    "        roc_curve(calib, x_test_, y_test)\n",
    "    results1.append(each)\n",
    "\n",
    "pd.DataFrame(results1).to_csv('results11.csv', index=False)\n",
    "return_results(pd.DataFrame(results1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Semi-Supervised Learning/ Self-training: select 50% of the positive\n",
    "class along with 50% of the negative class in the training set as labeled data\n",
    "and the rest as unlabelled data. You can select them randomly.   \n",
    "   \n",
    "A. Train an L1-penalized SVM to classify the labeled data Use normalized\n",
    "data. Choose the penalty parameter using 5 fold cross validation.   \n",
    "   \n",
    "B. Find the unlabeled data point that is the farthest to the decision boundary\n",
    "of the SVM. Let the SVM label it (ignore its true label), and add it to\n",
    "the labeled data, and retrain the SVM. Continue this process until all\n",
    "unlabeled data are used. Test the final SVM on the test data andthe\n",
    "average accuracy, precision, recall, F-score, and AUC, for both training\n",
    "and test sets over your M runs. Plot the ROC and report the confusion\n",
    "matrix for training and testing in one of the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_df(df):\n",
    "    df = shuffle(df) \n",
    "    M_df = df[df['Diagnosis']==1]\n",
    "    B_df = df[df['Diagnosis']==0]\n",
    "    \n",
    "    M_test = M_df.iloc[:int(0.20*len(M_df)), :]\n",
    "    M_train = M_df.iloc[int(0.20*len(M_df)):, :]\n",
    "    \n",
    "    B_test = B_df.iloc[:int(0.20*len(B_df)), :]\n",
    "    B_train = B_df.iloc[int(0.20*len(B_df)):, :]\n",
    "    \n",
    "    train_df = pd.concat([M_train, B_train])\n",
    "    test_df = pd.concat([M_test, B_test])\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lab_unlab_split(df):\n",
    "    df = shuffle(df) \n",
    "    M_df = df[df['Diagnosis']==1]\n",
    "    B_df = df[df['Diagnosis']==0]\n",
    "    \n",
    "    M_test = M_df.iloc[:int(0.50*len(M_df)), :]\n",
    "    M_train = M_df.iloc[int(0.50*len(M_df)):, :]\n",
    "    \n",
    "    B_test = B_df.iloc[:int(0.50*len(B_df)), :]\n",
    "    B_train = B_df.iloc[int(0.50*len(B_df)):, :]\n",
    "    \n",
    "    train_df = pd.concat([M_train, B_train])\n",
    "    test_df = pd.concat([M_test, B_test])\n",
    "    x_train = train_df.iloc[:, :-1]\n",
    "    y_train = train_df.iloc[:, -1]\n",
    "    x_test = test_df.iloc[:, :-1]\n",
    "    y_test = test_df.iloc[:, -1]\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = df.columns.values[1:]\n",
    "y_cols = df.columns.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_farthest_points(svc, x_test_):\n",
    "    dd = pd.DataFrame(x_test_)\n",
    "    dist = svc.decision_function(x_test_)\n",
    "    #w_norm = np.linalg.norm(svc.coef_)\n",
    "    #dist = y / w_norm\n",
    "    d = np.argmax(dist)\n",
    "    point = svc.predict([dd.iloc[d, :]])\n",
    "    \n",
    "    return d, dd.iloc[d,:], point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use normalized data. Report the average accuracy, precision, recall, F-score, and AUC, for both training and test sets \n",
    "#over your M runs. Plot the ROC and report the confusion matrix for training and testing in one of the runs.\n",
    "results2 = []\n",
    "\n",
    "for i in tqdm(range(30)):\n",
    "    #print('Run Number {}'.format(i))\n",
    "    each = []\n",
    "    \n",
    "    #Divide the initial df into 80-20 test-train\n",
    "    train_d, test_d = train_test_split_df(df)\n",
    "    \n",
    "    x_train_ = train_d.iloc[:, 1:]\n",
    "    y_train = train_d.iloc[:, 0]\n",
    "    x_test_ = test_d.iloc[:, 1:]\n",
    "    y_test = test_d.iloc[:, 0]\n",
    "    \n",
    "    scaler = preprocessing.Normalizer()\n",
    "    \n",
    "    train_data = pd.DataFrame(scaler.fit_transform(x_train_), columns=df.columns.values[1:])\n",
    "    test_data = pd.DataFrame(scaler.transform(x_test_), columns=df.columns.values[1:])\n",
    "    \n",
    "    train_data['Diagnosis'] = [int(i) for i in y_train]\n",
    "    test_data['Diagnosis'] = [int(j) for j in y_test]\n",
    "    \n",
    "    print(df.shape, train_data.shape, test_data.shape)\n",
    "    \n",
    "    #Concat train data, and divide into 50-50 labelled and unlabelled\n",
    "    #train_data = pd.concat([x_train, y_train])\n",
    "    lab_x, lab_y, unlab_x, unlab_y = lab_unlab_split(train_data)\n",
    "    times = len(unlab_x) + 1\n",
    "    \n",
    "    for j in range(times):\n",
    "        #Use labelled data as your training data and unlabelled data as your test data\n",
    "        print(lab_x.shape, lab_y.shape, unlab_x.shape, unlab_y.shape)\n",
    "        scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "        sv, results = linearSVC(lab_x, lab_y, scoring)\n",
    "        \n",
    "        if(j < (times-1)):\n",
    "            d, xx, yy = get_farthest_points(sv, unlab_x)\n",
    "            lab_x = lab_x.append(pd.Series(xx))\n",
    "            lab_y = lab_y.append(pd.Series(yy))\n",
    "            unlab_x.drop(unlab_x.index[d], inplace=True)\n",
    "            unlab_y.drop(unlab_y.index[d], inplace=True)\n",
    "\n",
    "    each.append(round(np.mean(results['mean_train_accuracy']), 2))\n",
    "    each.append(round(np.mean(results['mean_train_precision']), 2))\n",
    "    each.append(round(np.mean(results['mean_train_recall']), 2))\n",
    "    each.append(round(np.mean(results['mean_train_f1']), 2))\n",
    "    each.append(round(np.mean(results['mean_train_roc_auc']), 2))\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    x_test = test_data.iloc[:, :-1]\n",
    "    y_test = test_data.iloc[:, -1]\n",
    "    y_pred = sv.predict(x_test)\n",
    "    \n",
    "    each.append(round(metrics.accuracy_score(y_test, y_pred), 2))\n",
    "    each.append(round(metrics.recall_score(y_test, y_pred), 2))\n",
    "    each.append(round(metrics.precision_score(y_test, y_pred), 2))\n",
    "    each.append(round(metrics.f1_score(y_test, y_pred), 2))\n",
    "    each.append(round(metrics.roc_auc_score(y_test, y_pred), 2))\n",
    "    \n",
    "    #print('Avg. Train Accuracy {}'.format(round(np.mean(results['mean_train_accuracy']), 2)))\n",
    "    #print('Test Accuracy {}'.format(round(metrics.accuracy_score(y_test, y_pred), 2)))\n",
    "    \n",
    "    if(i == 29):\n",
    "        print('\\nFor Run Number {}'.format(i))\n",
    "        kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "        calib = CalibratedClassifierCV(sv, cv=kf)\n",
    "        \n",
    "        cnf_train = metrics.confusion_matrix(y_train, sv.predict(x_train))\n",
    "        print('Train Confusion Matrix')\n",
    "        print(cnf_train)\n",
    "        calib.fit(lab_x, lab_y)\n",
    "        roc_curve(calib, lab_x, lab_y)\n",
    "\n",
    "        cnf_test = metrics.confusion_matrix(y_test, y_pred)\n",
    "        print('\\nTest Confusion Matrix')\n",
    "        print(cnf_test)\n",
    "        roc_curve(calib, x_test, y_test)\n",
    "    \n",
    "    results2.append(each)\n",
    "    #print('\\n')\n",
    "\n",
    "pd.DataFrame(results2).to_csv('results12.csv', index=False)\n",
    "return_results(pd.DataFrame(results2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii. Unsupervised Learning: Run k-means algorithm on the whole training\n",
    "set. Ignore the labels of the data, and assume k = 2.   \n",
    "   \n",
    "A. Run the k-means algorithm multiple times. Make sure that you initialize\n",
    "the algoritm randomly. How do you make sure that the algorithm was\n",
    "not trapped in a local minimum?   \n",
    "   \n",
    "B. Compute the centers of the two clusters and find the closest 30 data\n",
    "points to each center. Read the true labels of those 30 data points and\n",
    "take a majority poll within them. The majority poll becomes the label\n",
    "predicted by k-means for the members of each cluster. Then compare the\n",
    "labels provided by k-means with the true labels of the training data and\n",
    "report the average accuracy, precision, recall, F-score, and AUC over M\n",
    "runs, and ROC and the confusion matrix for one of the runs.   \n",
    "   \n",
    "C. Classify test data based on their proximity to the centers of the clusters.\n",
    "Report the average accuracy, precision, recall, F-score, and AUC over M\n",
    "runs, and ROC and the confusion matrix for one of the runs for the test\n",
    "data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the k-means algorithm, you cannot entirely avoid local minima; you can only try to minimize your chances of getting one.\n",
    "\n",
    "A common hack used by many, and the one that can be done is setting n_init = 100 is to run K-means multiple times and then choosing the run that gives the lowest error. If you run this k^n times and then choose the best out of that, then you WOULD be guaranteed you're finding a global minima, but that's too time consuming to be practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_transform(kmeans, train_data):\n",
    "    k = kmeans.fit_transform(train_data)\n",
    "    res1 = []\n",
    "    res2 = []\n",
    "\n",
    "    for i in range(len(k)):\n",
    "        each = k[i]\n",
    "        which = np.argmin(each)\n",
    "        if(which==0): res1.append([i, 'cluster1', each[0]])\n",
    "        else: res2.append([i, 'cluster2', each[1]])\n",
    "    \n",
    "    top_cluster1 = pd.DataFrame(res1).sort_values([2], ascending=True)[:30].iloc[:, 0].reset_index(drop=True)\n",
    "    top_cluster2 = pd.DataFrame(res2).sort_values([2], ascending=True)[:30].iloc[:, 0].reset_index(drop=True)\n",
    "    \n",
    "    major_cluster = {'cluster1': {'0':0, '1':0}, 'cluster2': {'0':0, '1':0}}\n",
    "    for p in top_cluster1:\n",
    "        if(y_train[p] == 0):\n",
    "            major_cluster['cluster1']['0'] += 1\n",
    "        else:\n",
    "            major_cluster['cluster1']['1'] += 1\n",
    "\n",
    "    for q in top_cluster2:\n",
    "        if(y_train[q] == 0):\n",
    "            major_cluster['cluster2']['0'] += 1\n",
    "        else:\n",
    "            major_cluster['cluster2']['1'] += 1\n",
    "            \n",
    "    print(major_cluster)\n",
    "    clust1 = 0 if major_cluster['cluster1']['0'] > major_cluster['cluster1']['1'] else 1\n",
    "    clust2 = 0 if major_cluster['cluster2']['0'] > major_cluster['cluster2']['1'] else 1\n",
    "    \n",
    "    return clust1, clust2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_scores(y_train, y_pred, y_test, y_pred_test):\n",
    "    each = []\n",
    "    each.append(round(metrics.accuracy_score(y_train, y_pred), 2))\n",
    "    each.append(round(metrics.recall_score(y_train, y_pred), 2))\n",
    "    each.append(round(metrics.precision_score(y_train, y_pred), 2))\n",
    "    each.append(round(metrics.f1_score(y_train, y_pred), 2))\n",
    "    each.append(round(metrics.roc_auc_score(y_train, y_pred), 2))\n",
    "\n",
    "    each.append(round(metrics.accuracy_score(y_test, y_pred_test), 2))\n",
    "    each.append(round(metrics.recall_score(y_test, y_pred_test), 2))\n",
    "    each.append(round(metrics.precision_score(y_test, y_pred_test), 2))\n",
    "    each.append(round(metrics.f1_score(y_test, y_pred_test), 2))\n",
    "    each.append(round(metrics.roc_auc_score(y_test, y_pred_test), 2))\n",
    "\n",
    "    return each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the average accuracy, precision, recall, F-score, and AUC over M runs, \n",
    "#and ROC and the confusion matrix for one of the runs.\n",
    "results3 = []\n",
    "\n",
    "for i in tqdm(range(30)):\n",
    "    #Divide the initial df into 80-20 test-train\n",
    "    train_d, test_d = train_test_split_df(df)\n",
    "    \n",
    "    x_train_ = train_d.iloc[:, 1:].reset_index(drop=True)\n",
    "    y_train = train_d.iloc[:, 0].reset_index(drop=True)\n",
    "    x_test_ = test_d.iloc[:, 1:].reset_index(drop=True)\n",
    "    y_test = test_d.iloc[:, 0].reset_index(drop=True)\n",
    "\n",
    "    #scaler = preprocessing.Normalizer()\n",
    "\n",
    "    #train_data = pd.DataFrame(scaler.fit_transform(x_train_), columns=df.columns.values[1:])\n",
    "    #test_data = pd.DataFrame(scaler.fit_transform(x_test_), columns=df.columns.values[1:])\n",
    "    \n",
    "    train_data = x_train_\n",
    "    test_data = x_test_\n",
    "    \n",
    "    #Initiate the model\n",
    "    kmeans = KMeans(n_clusters=2, init='random', n_init=100)\n",
    "    kmeans.fit(train_data)\n",
    "    \n",
    "    clust1, clust2 = kmeans_transform(kmeans, train_data)\n",
    "    y_pred = pd.DataFrame(kmeans.predict(train_data)).replace({0:clust1, 1:clust2})\n",
    "    y_pred_test = pd.DataFrame(kmeans.predict(test_data)).replace({0:clust1, 1:clust2})\n",
    "    results3.append(give_scores(y_train, y_pred, y_test, y_pred_test))\n",
    "    \n",
    "    if(i == 29):\n",
    "        print('For Run Number {}'.format(i))\n",
    "        #kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "        #calib = CalibratedClassifierCV(kmeans, cv=kf)\n",
    "        #calib.fit(train_data, y_train)\n",
    "        \n",
    "        cnf_train = metrics.confusion_matrix(y_train, y_pred)\n",
    "        print('Train Confusion Matrix')\n",
    "        print(cnf_train)\n",
    "        #roc_curve(calib, train_data, y_train)\n",
    "\n",
    "        cnf_test = metrics.confusion_matrix(y_test, y_pred_test)\n",
    "        print('\\nTest Confusion Matrix')\n",
    "        print(cnf_test)\n",
    "        #roc_curve(calib, test_data, y_test)\n",
    "\n",
    "pd.DataFrame(results3).to_csv('results13.csv', index=False)\n",
    "return_results(pd.DataFrame(results3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iv. Spectral Clustering: Repeat 1(b)iii using spectral clustering, which is clustering based on kernels. Research what spectral clustering is. Use RBF\n",
    "kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invoke a Spectral Embedding and then Run a different KMeans model on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import SpectralEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the average accuracy, precision, recall, F-score, and AUC over M runs, \n",
    "#and ROC and the confusion matrix for one of the runs.\n",
    "results4 = []\n",
    "\n",
    "for i in tqdm(range(30)):\n",
    "    #Divide the initial df into 80-20 test-train\n",
    "    train_d, test_d = train_test_split_df(df)\n",
    "    \n",
    "    x_train_ = train_d.iloc[:, 1:].reset_index(drop=True)\n",
    "    y_train = train_d.iloc[:, 0].reset_index(drop=True)\n",
    "    x_test_ = test_d.iloc[:, 1:].reset_index(drop=True)\n",
    "    y_test = test_d.iloc[:, 0].reset_index(drop=True)\n",
    "    \n",
    "    #Invoking a Spectral Embedding and then invoking a new KMeans model with it\n",
    "    embed = SpectralEmbedding(n_components=30, affinity='rbf')\n",
    "    train_data = embed.fit_transform(x_train_)\n",
    "    test_data = embed.fit_transform(x_test_)\n",
    "    \n",
    "    #Initiate the model\n",
    "    kmeans = KMeans(n_clusters=2, init='random', n_init=100)\n",
    "    kmeans.fit(train_data)\n",
    "    \n",
    "    clust1, clust2 = kmeans_transform(kmeans, train_data)\n",
    "    y_pred = pd.DataFrame(kmeans.predict(train_data)).replace({0:clust1, 1:clust2})\n",
    "    y_pred_test = pd.DataFrame(kmeans.predict(test_data)).replace({0:clust1, 1:clust2})\n",
    "    results4.append(give_scores(y_train, y_pred, y_test, y_pred_test))\n",
    "    \n",
    "    if(i == 29):\n",
    "        print('For Run Number {}'.format(i))\n",
    "        #kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "        #calib = CalibratedClassifierCV(kmeans, cv=kf)\n",
    "        #calib.fit(train_data, y_train)\n",
    "        \n",
    "        cnf_train = metrics.confusion_matrix(y_train, y_pred)\n",
    "        print('Train Confusion Matrix')\n",
    "        print(cnf_train)\n",
    "        #roc_curve(calib, train_data, y_train)\n",
    "\n",
    "        cnf_test = metrics.confusion_matrix(y_test, y_pred_test)\n",
    "        print('\\nTest Confusion Matrix')\n",
    "        print(cnf_test)\n",
    "        #roc_curve(calib, test_data, y_test)\n",
    "\n",
    "pd.DataFrame(results4).to_csv('results14.csv', index=False)\n",
    "return_results(pd.DataFrame(results4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Active Learning Using Support Vector Machines   \n",
    "   \n",
    "(a) Download the banknote authentication Data Set from: https://archive.ics.uci.edu/ml/datasets/banknote+authentication. Choose 472 data points ran-\n",
    "domly as the test set, and the remaining 900 points as the training set. This is a\n",
    "binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Repeat each of the following two procedures 50 times. You will have 50 errors for\n",
    "90 SVMs per each procedure.   \n",
    "   \n",
    "i. Train a SVM with a pool of 10 randomly selected data points from the training\n",
    "set using linear kernel and L1 penalty. Select the penalty parameter using\n",
    "10-fold cross validation.2 Repeat this process by adding 10 other randomly\n",
    "selected data points to the pool, until you use all the 900 points. Do NOT\n",
    "replace the samples back into the training set at each step. Calculate the\n",
    "test error for each SVM. You will have 90 SVMs that were trained using 10,\n",
    "20, 30, ... , 900 data points and their 90 test errors. You have implemented\n",
    "passive learning.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Train a SVM with a pool of 10 randomly selected data points from the training\n",
    "set3 using linear kernel and L1 penalty. Select the parameters of the SVM\n",
    "with 10-fold cross validation. Choose the 10 closest data points in the training\n",
    "set to the hyperplane of the SVM4 and add them to the pool. Do not replace\n",
    "the samples back into the training set. Train a new SVM using the pool.\n",
    "Repeat this process until all training data is used. You will have 90 SVMs\n",
    "that were trained using 10, 20, 30,..., 900 data points and their 90 test errors.\n",
    "You have implemented active learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Average the 50 test errors for each of the incrementally trained 90 SVMs in 2(b)i\n",
    "and 2(b)ii. By doing so, you are performing a Monte Carlo simulation. Plot\n",
    "average test error versus number of training instances for both active and passive\n",
    "learners on the same figure and report your conclusions. Here, you are actually\n",
    "obtaining a learning curve by Monte-Carlo simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
