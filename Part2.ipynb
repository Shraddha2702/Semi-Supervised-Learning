{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from math import log10\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Active Learning Using Support Vector Machines   \n",
    "   \n",
    "(a) Download the banknote authentication Data Set from: https://archive.ics.uci.edu/ml/datasets/banknote+authentication. Choose 472 data points randomly as the test set, and the remaining 900 points as the training set. This is a\n",
    "binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dba.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.6661</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.6383</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.5228</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.4552</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variance  skewness  curtosis  entropy  class\n",
       "0   3.62160    8.6661   -2.8073 -0.44699      0\n",
       "1   4.54590    8.1674   -2.4586 -1.46210      0\n",
       "2   3.86600   -2.6383    1.9242  0.10645      0\n",
       "3   3.45660    9.5228   -4.0112 -3.59440      0\n",
       "4   0.32924   -4.4552    4.5718 -0.98880      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 5) (472, 5)\n"
     ]
    }
   ],
   "source": [
    "df = shuffle(df).reset_index(drop=True)\n",
    "test_data = df[:472]\n",
    "train_data = df[472:]\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_c_range(x_train, y_train):\n",
    "    c_ = np.logspace(-5, 8, 10)\n",
    "    scores = []\n",
    "    for c in c_:\n",
    "        svc = LinearSVC(penalty='l1', C=c, dual=False)\n",
    "        svc.fit(x_train, y_train)\n",
    "        scores.append(svc.score(x_train, y_train))\n",
    "    scores = np.array(scores)\n",
    "    ind = np.argwhere(scores > 0.9).flatten()\n",
    "    c_1 = c_[ind[0]]\n",
    "    c_2 = c_[ind[-1]]\n",
    "    return c_1, c_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Repeat each of the following two procedures 50 times. You will have 50 errors for\n",
    "90 SVMs per each procedure.   \n",
    "   \n",
    "i. Train a SVM with a pool of 10 randomly selected data points from the training\n",
    "set using linear kernel and L1 penalty. Select the penalty parameter using\n",
    "10-fold cross validation.2 Repeat this process by adding 10 other randomly\n",
    "selected data points to the pool, until you use all the 900 points. Do NOT\n",
    "replace the samples back into the training set at each step. Calculate the\n",
    "test error for each SVM. You will have 90 SVMs that were trained using 10,\n",
    "20, 30, ... , 900 data points and their 90 test errors. You have implemented\n",
    "passive learning.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4) (10,)\n",
      "Test Error 0.06999999999999995\n",
      "(20, 4) (20,)\n",
      "Test Error 0.030000000000000027\n",
      "(30, 4) (30,)\n",
      "Test Error 0.010000000000000009\n",
      "(40, 4) (40,)\n",
      "Test Error 0.020000000000000018\n",
      "(50, 4) (50,)\n",
      "Test Error 0.020000000000000018\n",
      "(60, 4) (60,)\n",
      "Test Error 0.030000000000000027\n",
      "(70, 4) (70,)\n",
      "Test Error 0.030000000000000027\n",
      "(80, 4) (80,)\n",
      "Test Error 0.020000000000000018\n",
      "(90, 4) (90,)\n",
      "Test Error 0.020000000000000018\n",
      "(100, 4) (100,)\n",
      "Test Error 0.020000000000000018\n",
      "(110, 4) (110,)\n",
      "Test Error 0.020000000000000018\n",
      "(120, 4) (120,)\n",
      "Test Error 0.010000000000000009\n",
      "(130, 4) (130,)\n",
      "Test Error 0.010000000000000009\n",
      "(140, 4) (140,)\n",
      "Test Error 0.010000000000000009\n",
      "(150, 4) (150,)\n",
      "Test Error 0.010000000000000009\n",
      "(160, 4) (160,)\n",
      "Test Error 0.010000000000000009\n",
      "(170, 4) (170,)\n",
      "Test Error 0.010000000000000009\n",
      "(180, 4) (180,)\n",
      "Test Error 0.010000000000000009\n",
      "(190, 4) (190,)\n",
      "Test Error 0.010000000000000009\n",
      "(200, 4) (200,)\n",
      "Test Error 0.010000000000000009\n",
      "(210, 4) (210,)\n",
      "Test Error 0.010000000000000009\n",
      "(220, 4) (220,)\n",
      "Test Error 0.010000000000000009\n",
      "(230, 4) (230,)\n",
      "Test Error 0.010000000000000009\n",
      "(240, 4) (240,)\n",
      "Test Error 0.010000000000000009\n",
      "(250, 4) (250,)\n",
      "Test Error 0.010000000000000009\n",
      "(260, 4) (260,)\n",
      "Test Error 0.010000000000000009\n",
      "(270, 4) (270,)\n",
      "Test Error 0.010000000000000009\n",
      "(280, 4) (280,)\n",
      "Test Error 0.010000000000000009\n",
      "(290, 4) (290,)\n",
      "Test Error 0.010000000000000009\n",
      "(300, 4) (300,)\n",
      "Test Error 0.010000000000000009\n",
      "(310, 4) (310,)\n",
      "Test Error 0.010000000000000009\n",
      "(320, 4) (320,)\n",
      "Test Error 0.010000000000000009\n",
      "(330, 4) (330,)\n",
      "Test Error 0.010000000000000009\n",
      "(340, 4) (340,)\n",
      "Test Error 0.010000000000000009\n",
      "(350, 4) (350,)\n",
      "Test Error 0.010000000000000009\n",
      "(360, 4) (360,)\n",
      "Test Error 0.010000000000000009\n",
      "(370, 4) (370,)\n",
      "Test Error 0.010000000000000009\n",
      "(380, 4) (380,)\n",
      "Test Error 0.010000000000000009\n",
      "(390, 4) (390,)\n",
      "Test Error 0.010000000000000009\n",
      "(400, 4) (400,)\n",
      "Test Error 0.010000000000000009\n",
      "(410, 4) (410,)\n",
      "Test Error 0.010000000000000009\n",
      "(420, 4) (420,)\n",
      "Test Error 0.010000000000000009\n",
      "(430, 4) (430,)\n",
      "Test Error 0.010000000000000009\n",
      "(440, 4) (440,)\n",
      "Test Error 0.010000000000000009\n",
      "(450, 4) (450,)\n",
      "Test Error 0.010000000000000009\n",
      "(460, 4) (460,)\n",
      "Test Error 0.010000000000000009\n",
      "(470, 4) (470,)\n",
      "Test Error 0.010000000000000009\n",
      "(480, 4) (480,)\n",
      "Test Error 0.010000000000000009\n",
      "(490, 4) (490,)\n",
      "Test Error 0.010000000000000009\n",
      "(500, 4) (500,)\n",
      "Test Error 0.010000000000000009\n",
      "(510, 4) (510,)\n",
      "Test Error 0.010000000000000009\n",
      "(520, 4) (520,)\n",
      "Test Error 0.010000000000000009\n",
      "(530, 4) (530,)\n",
      "Test Error 0.010000000000000009\n",
      "(540, 4) (540,)\n",
      "Test Error 0.010000000000000009\n",
      "(550, 4) (550,)\n",
      "Test Error 0.010000000000000009\n",
      "(560, 4) (560,)\n",
      "Test Error 0.010000000000000009\n",
      "(570, 4) (570,)\n",
      "Test Error 0.010000000000000009\n",
      "(580, 4) (580,)\n",
      "Test Error 0.010000000000000009\n",
      "(590, 4) (590,)\n",
      "Test Error 0.010000000000000009\n",
      "(600, 4) (600,)\n",
      "Test Error 0.010000000000000009\n",
      "(610, 4) (610,)\n",
      "Test Error 0.010000000000000009\n",
      "(620, 4) (620,)\n",
      "Test Error 0.010000000000000009\n",
      "(630, 4) (630,)\n",
      "Test Error 0.010000000000000009\n",
      "(640, 4) (640,)\n",
      "Test Error 0.010000000000000009\n",
      "(650, 4) (650,)\n",
      "Test Error 0.010000000000000009\n",
      "(660, 4) (660,)\n",
      "Test Error 0.010000000000000009\n",
      "(670, 4) (670,)\n",
      "Test Error 0.010000000000000009\n",
      "(680, 4) (680,)\n",
      "Test Error 0.010000000000000009\n",
      "(690, 4) (690,)\n",
      "Test Error 0.010000000000000009\n",
      "(700, 4) (700,)\n",
      "Test Error 0.010000000000000009\n",
      "(710, 4) (710,)\n",
      "Test Error 0.010000000000000009\n",
      "(720, 4) (720,)\n",
      "Test Error 0.010000000000000009\n",
      "(730, 4) (730,)\n",
      "Test Error 0.010000000000000009\n",
      "(740, 4) (740,)\n",
      "Test Error 0.010000000000000009\n",
      "(750, 4) (750,)\n",
      "Test Error 0.010000000000000009\n",
      "(760, 4) (760,)\n",
      "Test Error 0.010000000000000009\n",
      "(770, 4) (770,)\n",
      "Test Error 0.010000000000000009\n",
      "(780, 4) (780,)\n",
      "Test Error 0.010000000000000009\n",
      "(790, 4) (790,)\n",
      "Test Error 0.010000000000000009\n",
      "(800, 4) (800,)\n",
      "Test Error 0.010000000000000009\n",
      "(810, 4) (810,)\n",
      "Test Error 0.010000000000000009\n",
      "(820, 4) (820,)\n",
      "Test Error 0.010000000000000009\n",
      "(830, 4) (830,)\n",
      "Test Error 0.010000000000000009\n",
      "(840, 4) (840,)\n",
      "Test Error 0.010000000000000009\n",
      "(850, 4) (850,)\n",
      "Test Error 0.010000000000000009\n",
      "(860, 4) (860,)\n",
      "Test Error 0.010000000000000009\n",
      "(870, 4) (870,)\n",
      "Test Error 0.010000000000000009\n",
      "(880, 4) (880,)\n",
      "Test Error 0.010000000000000009\n",
      "(890, 4) (890,)\n",
      "Test Error 0.010000000000000009\n",
      "(900, 4) (900,)\n",
      "Test Error 0.010000000000000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [04:57<00:00, 297.11s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i in tqdm(range(1)):\n",
    "    train_data = shuffle(train_data)\n",
    "    test_data = shuffle(test_data)\n",
    "    x_test = test_data.iloc[:, :-1]\n",
    "    y_test = test_data.iloc[:, -1]\n",
    "    \n",
    "    each = []\n",
    "    \n",
    "    for j in range(0, 900, 10):\n",
    "        x_train = train_data.iloc[:10+j, :-1]\n",
    "        y_train = train_data.iloc[:10+j, -1]\n",
    "        #print(x_train.shape, y_train.shape)\n",
    "        \n",
    "        c_l, c_h = get_c_range(x_train, y_train)\n",
    "        parameters = {'C':np.logspace(log10(c_l), log10(c_h), 20)}\n",
    "\n",
    "        svc = LinearSVC(penalty='l1', dual=False)\n",
    "        kf = KFold(n_splits=10, shuffle=True)\n",
    "        clf = GridSearchCV(svc, parameters, cv=kf, scoring='accuracy', return_train_score=True)\n",
    "        clf.fit(x_train, y_train)\n",
    "\n",
    "        #calculating test accuracy\n",
    "        sv = clf.best_estimator_\n",
    "        y_pred = sv.predict(x_test)\n",
    "\n",
    "        each.append(1 - metrics.accuracy_score(y_test, y_pred))\n",
    "        print('Test Error {}'.format(1 - round(metrics.accuracy_score(y_test, y_pred), 2)))\n",
    "    results.append(np.mean(each))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Train a SVM with a pool of 10 randomly selected data points from the training\n",
    "set3 using linear kernel and L1 penalty. Select the parameters of the SVM\n",
    "with 10-fold cross validation. Choose the 10 closest data points in the training\n",
    "set to the hyperplane of the SVM4 and add them to the pool. Do not replace\n",
    "the samples back into the training set. Train a new SVM using the pool.\n",
    "Repeat this process until all training data is used. You will have 90 SVMs\n",
    "that were trained using 10, 20, 30,..., 900 data points and their 90 test errors.\n",
    "You have implemented active learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4) (10,) (890, 5)\n",
      "Test Error 0.09999999999999998\n",
      "(20, 4) (20,) (880, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(30, 4) (30,) (870, 5)\n",
      "Test Error 0.020000000000000018\n",
      "(40, 4) (40,) (860, 5)\n",
      "Test Error 0.030000000000000027\n",
      "(50, 4) (50,) (850, 5)\n",
      "Test Error 0.020000000000000018\n",
      "(60, 4) (60,) (840, 5)\n",
      "Test Error 0.020000000000000018\n",
      "(70, 4) (70,) (830, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(80, 4) (80,) (820, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(90, 4) (90,) (810, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(100, 4) (100,) (800, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(110, 4) (110,) (790, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(120, 4) (120,) (780, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(130, 4) (130,) (770, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(140, 4) (140,) (760, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(150, 4) (150,) (750, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(160, 4) (160,) (740, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(170, 4) (170,) (730, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(180, 4) (180,) (720, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(190, 4) (190,) (710, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(200, 4) (200,) (700, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(210, 4) (210,) (690, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(220, 4) (220,) (680, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(230, 4) (230,) (670, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(240, 4) (240,) (660, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(250, 4) (250,) (650, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(260, 4) (260,) (640, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(270, 4) (270,) (630, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(280, 4) (280,) (620, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(290, 4) (290,) (610, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(300, 4) (300,) (600, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(310, 4) (310,) (590, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(320, 4) (320,) (580, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(330, 4) (330,) (570, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(340, 4) (340,) (560, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(350, 4) (350,) (550, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(360, 4) (360,) (540, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(370, 4) (370,) (530, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(380, 4) (380,) (520, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(390, 4) (390,) (510, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(400, 4) (400,) (500, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(410, 4) (410,) (490, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(420, 4) (420,) (480, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(430, 4) (430,) (470, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(440, 4) (440,) (460, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(450, 4) (450,) (450, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(460, 4) (460,) (440, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(470, 4) (470,) (430, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(480, 4) (480,) (420, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(490, 4) (490,) (410, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(500, 4) (500,) (400, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(510, 4) (510,) (390, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(520, 4) (520,) (380, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(530, 4) (530,) (370, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(540, 4) (540,) (360, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(550, 4) (550,) (350, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(560, 4) (560,) (340, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(570, 4) (570,) (330, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(580, 4) (580,) (320, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(590, 4) (590,) (310, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(600, 4) (600,) (300, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(610, 4) (610,) (290, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(620, 4) (620,) (280, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(630, 4) (630,) (270, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(640, 4) (640,) (260, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(650, 4) (650,) (250, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(660, 4) (660,) (240, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(670, 4) (670,) (230, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(680, 4) (680,) (220, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(690, 4) (690,) (210, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(700, 4) (700,) (200, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(710, 4) (710,) (190, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(720, 4) (720,) (180, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(730, 4) (730,) (170, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(740, 4) (740,) (160, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(750, 4) (750,) (150, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(760, 4) (760,) (140, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(770, 4) (770,) (130, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(780, 4) (780,) (120, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(790, 4) (790,) (110, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(800, 4) (800,) (100, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(810, 4) (810,) (90, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(820, 4) (820,) (80, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(830, 4) (830,) (70, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(840, 4) (840,) (60, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(850, 4) (850,) (50, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(860, 4) (860,) (40, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(870, 4) (870,) (30, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(880, 4) (880,) (20, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(890, 4) (890,) (10, 5)\n",
      "Test Error 0.010000000000000009\n",
      "(900, 4) (900,) (0, 5)\n",
      "Test Error 0.010000000000000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [04:50<00:00, 290.43s/it]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results1 = []\n",
    "\n",
    "for i in tqdm(range(1)):\n",
    "    train_data = shuffle(train_data)\n",
    "    test_data = shuffle(test_data)\n",
    "    \n",
    "    x_train_taken = train_data.iloc[:10, :-1].reset_index(drop=True)\n",
    "    y_train_taken = train_data.iloc[:10, -1].reset_index(drop=True)\n",
    "    train_nottak = train_data.iloc[10:, :].reset_index(drop=True)\n",
    "    \n",
    "    x_test = test_data.iloc[:, :-1]\n",
    "    y_test = test_data.iloc[:, -1]\n",
    "    \n",
    "    each = []\n",
    "    \n",
    "    for j in range(90):\n",
    "        print(x_train_taken.shape, y_train_taken.shape, train_nottak.shape)\n",
    "        \n",
    "        c_l, c_h = get_c_range(x_train_taken, y_train_taken)\n",
    "        parameters = {'C':np.logspace(log10(c_l), log10(c_h), 20)}\n",
    "\n",
    "        svc = LinearSVC(penalty='l1', dual=False)\n",
    "        kf = KFold(n_splits=10, shuffle=True)\n",
    "        clf = GridSearchCV(svc, parameters, cv=kf, scoring='accuracy', return_train_score=True)\n",
    "        clf.fit(x_train_taken, y_train_taken)\n",
    "\n",
    "        #calculating test accuracy\n",
    "        sv = clf.best_estimator_\n",
    "        \n",
    "        if(j < 89):\n",
    "            y = sv.decision_function(train_nottak.iloc[:, :-1])\n",
    "            w_norm = np.linalg.norm(sv.coef_)\n",
    "            dist = y / w_norm\n",
    "            dist = [abs(each) for each in dist]\n",
    "            train_nottak['dist'] = dist\n",
    "            train_nottak = train_nottak.sort(['dist'], ascending=True)\n",
    "            x_train_taken = x_train_taken.append(train_nottak.iloc[:10, :-2])\n",
    "            y_train_taken = y_train_taken.append(train_nottak.iloc[:10, -2])\n",
    "            train_nottak = train_nottak.iloc[10:, :-1]\n",
    "        \n",
    "        y_pred = sv.predict(x_test)\n",
    "        each.append(round(1 - metrics.accuracy_score(y_test, y_pred), 2))\n",
    "        print('Test Error {}'.format(1 - round(metrics.accuracy_score(y_test, y_pred), 2)))\n",
    "    results1.append(np.mean(each))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Average the 50 test errors for each of the incrementally trained 90 SVMs in 2(b)i\n",
    "and 2(b)ii. By doing so, you are performing a Monte Carlo simulation. Plot\n",
    "average test error versus number of training instances for both active and passive\n",
    "learners on the same figure and report your conclusions. Here, you are actually\n",
    "obtaining a learning curve by Monte-Carlo simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-766c096299c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnum_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m901\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'passive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'active'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'number of training instances'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   3159\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3160\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3161\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3162\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3163\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwashold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1817\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[0;32m   1818\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1819\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1820\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1821\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_alias_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1383\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m             \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    379\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    357\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'plot'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"x and y must have same first dimension\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"x and y can be no greater than 2-D\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27ac7891e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_train = range(10, 901, 10)\n",
    "\n",
    "plt.plot(num_train, results, label='passive')\n",
    "plt.plot(num_train, results1, label='active')\n",
    "plt.xlabel('number of training instances')\n",
    "plt.ylabel('average test error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = results.values - results1.values\n",
    "plt.plot(num_train, diff)\n",
    "plt.xlabel('number of training instances')\n",
    "plt.ylabel('test error difference')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "When the number of training instances is small, active learners perform better. As the number of training instances increase, passive learners outperform active learners. When the number of training instances is sufficiently large, the performace active learners and passive learners are basicly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
